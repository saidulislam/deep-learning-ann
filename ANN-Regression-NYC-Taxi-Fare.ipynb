{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fares dataset\n",
    "The <a href='https://www.kaggle.com/c/new-york-city-taxi-fare-prediction'>Kaggle competition</a> provides a dataset with about 55 million records. The data contains only the pickup date & time, the latitude & longitude (GPS coordinates) of the pickup and dropoff locations, and the number of passengers. It is up to the contest participant to extract any further information. For instance, does the time of day matter? The day of the week? How do we determine the distance traveled from pairs of GPS coordinates?\n",
    "\n",
    "For this exercise I have whittled the dataset down to just 120,000 records from April 11 to April 24, 2010. The records are randomly sorted. I will show how to calculate distance from GPS coordinates, and how to create a pandas datatime object from a text column. This will let us quickly get information like day of the week, am vs. pm, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Read and analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>fare_class</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-19 08:17:56 UTC</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.992365</td>\n",
       "      <td>40.730521</td>\n",
       "      <td>-73.975499</td>\n",
       "      <td>40.744746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-17 15:43:53 UTC</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990078</td>\n",
       "      <td>40.740558</td>\n",
       "      <td>-73.974232</td>\n",
       "      <td>40.744114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2010-04-17 11:23:26 UTC</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994149</td>\n",
       "      <td>40.751118</td>\n",
       "      <td>-73.960064</td>\n",
       "      <td>40.766235</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2010-04-11 21:25:03 UTC</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990485</td>\n",
       "      <td>40.756422</td>\n",
       "      <td>-73.971205</td>\n",
       "      <td>40.748192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2010-04-17 02:19:01 UTC</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.990976</td>\n",
       "      <td>40.734202</td>\n",
       "      <td>-73.905956</td>\n",
       "      <td>40.743115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime  fare_amount  fare_class  pickup_longitude  \\\n",
       "0  2010-04-19 08:17:56 UTC          6.5           0        -73.992365   \n",
       "1  2010-04-17 15:43:53 UTC          6.9           0        -73.990078   \n",
       "2  2010-04-17 11:23:26 UTC         10.1           1        -73.994149   \n",
       "3  2010-04-11 21:25:03 UTC          8.9           0        -73.990485   \n",
       "4  2010-04-17 02:19:01 UTC         19.7           1        -73.990976   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.730521         -73.975499         40.744746                1  \n",
       "1        40.740558         -73.974232         40.744114                1  \n",
       "2        40.751118         -73.960064         40.766235                2  \n",
       "3        40.756422         -73.971205         40.748192                1  \n",
       "4        40.734202         -73.905956         40.743115                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/NYCTaxiFares.csv')\n",
    "df.head() # check the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"fare_amount\" is the column we'll predict. Let's check the distribution of this column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120000.000000\n",
       "mean         10.040326\n",
       "std           7.500134\n",
       "min           2.500000\n",
       "25%           5.700000\n",
       "50%           7.700000\n",
       "75%          11.300000\n",
       "max          49.900000\n",
       "Name: fare_amount, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fare_amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that fares range from \\\\$2.50 to \\\\$49.90, with a mean of \\\\$10.04 and a median of \\\\$7.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Go through the feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the distance traveled\n",
    "The <a href='https://en.wikipedia.org/wiki/Haversine_formula'>haversine formula</a> calculates the distance on a sphere between two sets of GPS coordinates.<br>\n",
    "Here we assign latitude values with $\\varphi$ (phi) and longitude with $\\lambda$ (lambda).\n",
    "\n",
    "The distance formula works out to\n",
    "\n",
    "${\\displaystyle d=2r\\arcsin \\left({\\sqrt {\\sin ^{2}\\left({\\frac {\\varphi _{2}-\\varphi _{1}}{2}}\\right)+\\cos(\\varphi _{1})\\:\\cos(\\varphi _{2})\\:\\sin ^{2}\\left({\\frac {\\lambda _{2}-\\lambda _{1}}{2}}\\right)}}\\right)}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\begin{split} r&: \\textrm {radius of the sphere (Earth's radius averages 6371 km)}\\\\\n",
    "\\varphi_1, \\varphi_2&: \\textrm {latitudes of point 1 and point 2}\\\\\n",
    "\\lambda_1, \\lambda_2&: \\textrm {longitudes of point 1 and point 2}\\end{split}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, lat_1, long_1, lat_2, long_2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    earth_r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi_1 = np.radians(df[lat_1])\n",
    "    phi_2 = np.radians(df[lat_2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat_2]-df[lat_1])\n",
    "    delta_lambda = np.radians(df[long_2]-df[long_1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    k_distance = (earth_r * c) # in kilometers\n",
    "    m_distance = k_distance * 0.621371 # in miles\n",
    "\n",
    "    return m_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>fare_class</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>dist_miles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-19 08:17:56 UTC</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.992365</td>\n",
       "      <td>40.730521</td>\n",
       "      <td>-73.975499</td>\n",
       "      <td>40.744746</td>\n",
       "      <td>1</td>\n",
       "      <td>1.321228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-17 15:43:53 UTC</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990078</td>\n",
       "      <td>40.740558</td>\n",
       "      <td>-73.974232</td>\n",
       "      <td>40.744114</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2010-04-17 11:23:26 UTC</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994149</td>\n",
       "      <td>40.751118</td>\n",
       "      <td>-73.960064</td>\n",
       "      <td>40.766235</td>\n",
       "      <td>2</td>\n",
       "      <td>2.067154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2010-04-11 21:25:03 UTC</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990485</td>\n",
       "      <td>40.756422</td>\n",
       "      <td>-73.971205</td>\n",
       "      <td>40.748192</td>\n",
       "      <td>1</td>\n",
       "      <td>1.158316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2010-04-17 02:19:01 UTC</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.990976</td>\n",
       "      <td>40.734202</td>\n",
       "      <td>-73.905956</td>\n",
       "      <td>40.743115</td>\n",
       "      <td>1</td>\n",
       "      <td>4.493333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime  fare_amount  fare_class  pickup_longitude  \\\n",
       "0  2010-04-19 08:17:56 UTC          6.5           0        -73.992365   \n",
       "1  2010-04-17 15:43:53 UTC          6.9           0        -73.990078   \n",
       "2  2010-04-17 11:23:26 UTC         10.1           1        -73.994149   \n",
       "3  2010-04-11 21:25:03 UTC          8.9           0        -73.990485   \n",
       "4  2010-04-17 02:19:01 UTC         19.7           1        -73.990976   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0        40.730521         -73.975499         40.744746                1   \n",
       "1        40.740558         -73.974232         40.744114                1   \n",
       "2        40.751118         -73.960064         40.766235                2   \n",
       "3        40.756422         -73.971205         40.748192                1   \n",
       "4        40.734202         -73.905956         40.743115                1   \n",
       "\n",
       "   dist_miles  \n",
       "0    1.321228  \n",
       "1    0.865139  \n",
       "2    2.067154  \n",
       "3    1.158316  \n",
       "4    4.493333  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dist_miles'] = haversine_distance(df,'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of our newly created column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120000.000000\n",
       "mean          2.064294\n",
       "std           2.073518\n",
       "min           0.006343\n",
       "25%           0.817990\n",
       "50%           1.390059\n",
       "75%           2.506961\n",
       "max          17.924295\n",
       "Name: dist_miles, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dist_miles'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a datetime column and derive useful data\n",
    "By creating a datetime object, we can extract information like \"day of the week\", \"am vs. pm\" etc.\n",
    "Note that the data was saved in UTC time. Our data falls in April of 2010 which occurred during Daylight Savings Time in New York. For that reason, we'll make an adjustment to EDT using UTC-4 (subtracting four hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>fare_class</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>dist_miles</th>\n",
       "      <th>EDTdate</th>\n",
       "      <th>Hour</th>\n",
       "      <th>AMorPM</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-19 08:17:56 UTC</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.992365</td>\n",
       "      <td>40.730521</td>\n",
       "      <td>-73.975499</td>\n",
       "      <td>40.744746</td>\n",
       "      <td>1</td>\n",
       "      <td>1.321228</td>\n",
       "      <td>2010-04-19 04:17:56</td>\n",
       "      <td>4</td>\n",
       "      <td>am</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-17 15:43:53 UTC</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990078</td>\n",
       "      <td>40.740558</td>\n",
       "      <td>-73.974232</td>\n",
       "      <td>40.744114</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865139</td>\n",
       "      <td>2010-04-17 11:43:53</td>\n",
       "      <td>11</td>\n",
       "      <td>am</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2010-04-17 11:23:26 UTC</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994149</td>\n",
       "      <td>40.751118</td>\n",
       "      <td>-73.960064</td>\n",
       "      <td>40.766235</td>\n",
       "      <td>2</td>\n",
       "      <td>2.067154</td>\n",
       "      <td>2010-04-17 07:23:26</td>\n",
       "      <td>7</td>\n",
       "      <td>am</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2010-04-11 21:25:03 UTC</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990485</td>\n",
       "      <td>40.756422</td>\n",
       "      <td>-73.971205</td>\n",
       "      <td>40.748192</td>\n",
       "      <td>1</td>\n",
       "      <td>1.158316</td>\n",
       "      <td>2010-04-11 17:25:03</td>\n",
       "      <td>17</td>\n",
       "      <td>pm</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2010-04-17 02:19:01 UTC</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.990976</td>\n",
       "      <td>40.734202</td>\n",
       "      <td>-73.905956</td>\n",
       "      <td>40.743115</td>\n",
       "      <td>1</td>\n",
       "      <td>4.493333</td>\n",
       "      <td>2010-04-16 22:19:01</td>\n",
       "      <td>22</td>\n",
       "      <td>pm</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime  fare_amount  fare_class  pickup_longitude  \\\n",
       "0  2010-04-19 08:17:56 UTC          6.5           0        -73.992365   \n",
       "1  2010-04-17 15:43:53 UTC          6.9           0        -73.990078   \n",
       "2  2010-04-17 11:23:26 UTC         10.1           1        -73.994149   \n",
       "3  2010-04-11 21:25:03 UTC          8.9           0        -73.990485   \n",
       "4  2010-04-17 02:19:01 UTC         19.7           1        -73.990976   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0        40.730521         -73.975499         40.744746                1   \n",
       "1        40.740558         -73.974232         40.744114                1   \n",
       "2        40.751118         -73.960064         40.766235                2   \n",
       "3        40.756422         -73.971205         40.748192                1   \n",
       "4        40.734202         -73.905956         40.743115                1   \n",
       "\n",
       "   dist_miles             EDTdate  Hour AMorPM Weekday  \n",
       "0    1.321228 2010-04-19 04:17:56     4     am     Mon  \n",
       "1    0.865139 2010-04-17 11:43:53    11     am     Sat  \n",
       "2    2.067154 2010-04-17 07:23:26     7     am     Sat  \n",
       "3    1.158316 2010-04-11 17:25:03    17     pm     Sun  \n",
       "4    4.493333 2010-04-16 22:19:01    22     pm     Fri  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EDTdate'] = pd.to_datetime(df['pickup_datetime'].str[:19]) - pd.Timedelta(hours=4)\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour']<12,'am','pm')\n",
    "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate categorical columns from continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude',\n",
       "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
       "       'passenger_count', 'dist_miles', 'EDTdate', 'Hour', 'AMorPM',\n",
       "       'Weekday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Hour', 'AMorPM', 'Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'passenger_count', 'dist_miles']\n",
    "\n",
    "# we entered the continuous columns explicitly because there are columns we're not running through the model \n",
    "# (pickup_datetime and EDTdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = ['fare_amount']  # this column contains the labels, the values we are predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> If you plan to use all of the columns in the data table, there's a shortcut to grab the remaining continuous columns:<br>\n",
    "<pre style='background-color:rgb(217,237,247)'>cont_cols = [col for col in df.columns if col not in cat_cols + y_col]</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorify\n",
    "Pandas offers a <a href='https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html'><strong>category dtype</strong></a> for converting categorical values to numerical codes. A dataset containing months of the year will be assigned 12 codes, one for each month. These will usually be the integers 0 to 11. Pandas replaces the column values with codes, and retains an index list of category values. In the steps ahead we'll call the categorical values \"names\" and the encodings \"codes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our three categorical columns ['Hour', 'AMorPM', 'Weekday'] to category dtypes.\n",
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_datetime              object\n",
       "fare_amount                 float64\n",
       "fare_class                    int64\n",
       "pickup_longitude            float64\n",
       "pickup_latitude             float64\n",
       "dropoff_longitude           float64\n",
       "dropoff_latitude            float64\n",
       "passenger_count               int64\n",
       "dist_miles                  float64\n",
       "EDTdate              datetime64[ns]\n",
       "Hour                       category\n",
       "AMorPM                     category\n",
       "Weekday                    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets print and check all the data types in data frame\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4\n",
       "1    11\n",
       "2     7\n",
       "3    17\n",
       "4    22\n",
       "Name: Hour, dtype: category\n",
       "Categories (24, int64): [0, 1, 2, 3, ..., 20, 21, 22, 23]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check some df['Hour'] values\n",
    "df['Hour'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our categorical names are the integers 0 through 23, for a total of 24 unique categories. These values also correspond to the codes assigned to each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    am\n",
       "1    am\n",
       "2    am\n",
       "3    pm\n",
       "4    pm\n",
       "Name: AMorPM, dtype: category\n",
       "Categories (2, object): [am, pm]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check some df['AMorPM'] values\n",
    "df['AMorPM'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Mon\n",
       "1    Sat\n",
       "2    Sat\n",
       "3    Sun\n",
       "4    Fri\n",
       "Name: Weekday, dtype: category\n",
       "Categories (7, object): [Fri, Mon, Sat, Sun, Thu, Tue, Wed]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check some df['Weekday'] values\n",
    "df['Weekday'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the category names with Series.cat.categories or just the codes with Series.cat.codes. This makes more sense when we look at df['AMorPM'] or others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['am', 'pm'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AMorPM'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weekday'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    3\n",
       "4    0\n",
       "dtype: int8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weekday'].head().cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>NOTE: </strong>NaN values in categorical data are assigned a code of -1. We don't have any in this particular dataset.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to combine the three categorical columns into one input array using numpy.stack We don't want the Series index, just the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr = df['Hour'].cat.codes.values\n",
    "ampm = df['AMorPM'].cat.codes.values\n",
    "wkdy = df['Weekday'].cat.codes.values\n",
    "\n",
    "cats = np.stack([hr, ampm, wkdy], 1)\n",
    "\n",
    "# all of the above can also be done in one line (more pythonic way)\n",
    "# cats = np.stack([df[col].cat.codes.values for col in cat_cols], 1)\n",
    "\n",
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  1],\n",
       "       [11,  0,  2],\n",
       "       [ 7,  0,  2],\n",
       "       [17,  1,  3],\n",
       "       [22,  1,  0]], dtype=int8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints only 5 rows\n",
    "cats[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert numpy arrays to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Machine Learning related work, we need to convert our data into tensors. We start with numpy and then to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  0,  1],\n",
       "        [11,  0,  2],\n",
       "        [ 7,  0,  2],\n",
       "        [17,  1,  3],\n",
       "        [22,  1,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical variables to a tensors\n",
    "cats = torch.tensor(cats, dtype=torch.int64)\n",
    "\n",
    "cats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 40.7305, -73.9924,  40.7447, -73.9755,   1.0000,   1.3212],\n",
       "        [ 40.7406, -73.9901,  40.7441, -73.9742,   1.0000,   0.8651],\n",
       "        [ 40.7511, -73.9941,  40.7662, -73.9601,   2.0000,   2.0672],\n",
       "        [ 40.7564, -73.9905,  40.7482, -73.9712,   1.0000,   1.1583],\n",
       "        [ 40.7342, -73.9910,  40.7431, -73.9060,   1.0000,   4.4933]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert continuous variables to numpy and then to tensors\n",
    "conts = np.stack([df[col].values for col in cont_cols], 1)\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "conts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed all of our continuous variables into the model as a tensor. Note that we're not normalizing the values here; we'll let the model perform this step.\n",
    "<div class=\"alert alert-info\"><strong>NOTE:</strong> We have to store <tt>conts</tt> and <tt>y / labels (the value we are predicting)</tt> as Float (float32) tensors, not Double (float64) in order for batch normalization to work properly.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000],\n",
       "        [ 6.9000],\n",
       "        [10.1000],\n",
       "        [ 8.9000],\n",
       "        [19.7000]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to a tensor\n",
    "y = torch.tensor(df[y_col].values, dtype=torch.float).reshape(-1,1)\n",
    "\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120000, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120000, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120000, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for relationships between categories to be captured. Perhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.\n",
    "\n",
    "Source: https://www.fast.ai/2018/04/29/categorical-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the path you have to go through for category embedding.\n",
    "\n",
    "![title](images/category-embedding.png)\n",
    "Image Source: https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set an embedding size\n",
    "The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 12), (2, 1), (7, 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will set embedding sizes for Hours, AMvsPM and Weekdays\n",
    "cat_szs = [len(df[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a TabularModel\n",
    "This somewhat follows the <a href='https://docs.fast.ai/tabular.models.html'>fast.ai library</a> The goal is to define a model based on the number of continuous columns (given by <tt>conts.shape[1]</tt>) plus the number of categorical columns and their embeddings (given by <tt>len(emb_szs)</tt> and <tt>emb_szs</tt> respectively). The output would either be a regression (a single float value), or a classification (a group of bins and their softmax values). For this exercise our output will be a single regression value. Note that we'll assume our data contains both categorical and continuous data. You can add boolean parameters to your own model class to handle a variety of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Let's walk through the steps we're about to take. See below for more detailed illustrations of the steps.</strong><br>\n",
    "\n",
    "1. Extend the base Module class, set up the following parameters:\n",
    "   * <tt>emb_szs: </tt>list of tuples: each categorical variable size is paired with an embedding size\n",
    "   * <tt>n_cont:  </tt>int: number of continuous variables\n",
    "   * <tt>out_sz:  </tt>int: output size\n",
    "   * <tt>layers:  </tt>list of ints: layer sizes\n",
    "   * <tt>p:       </tt>float: dropout probability for each layer (for simplicity we'll use the same value throughout)\n",
    "   \n",
    "<tt><font color=black>class TabularModel(nn.Module):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;def \\_\\_init\\_\\_(self, emb_szs, n_cont, out_sz, layers, p=0.5):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().\\_\\_init\\_\\_()</font></tt><br>\n",
    "\n",
    "2. Set up the embedded layers with <a href='https://pytorch.org/docs/stable/nn.html#modulelist'><tt><strong>torch.nn.ModuleList()</strong></tt></a> and <a href='https://pytorch.org/docs/stable/nn.html#embedding'><tt><strong>torch.nn.Embedding()</strong></tt></a><br>Categorical data will be filtered through these Embeddings in the forward section.<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])</font></tt><br><br>\n",
    "3. Set up a dropout function for the embeddings with <a href='https://pytorch.org/docs/stable/nn.html#dropout'><tt><strong>torch.nn.Dropout()</strong></tt></a> The default p-value=0.5<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.emb_drop = nn.Dropout(emb_drop)</font></tt><br><br>\n",
    "4. Set up a normalization function for the continuous variables with <a href='https://pytorch.org/docs/stable/nn.html#batchnorm1d'><tt><strong>torch.nn.BatchNorm1d()</strong></tt></a><br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.bn_cont = nn.BatchNorm1d(n_cont)</font></tt><br><br>\n",
    "5. Set up a sequence of neural network layers where each level includes a Linear function, an activation function (we'll use <a href='https://pytorch.org/docs/stable/nn.html#relu'><strong>ReLU</strong></a>), a normalization step, and a dropout layer. We'll combine the list of layers with <a href='https://pytorch.org/docs/stable/nn.html#sequential'><tt><strong>torch.nn.Sequential()</strong></tt></a><br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;self.bn_cont = nn.BatchNorm1d(n_cont)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;layerlist = []<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;n_emb = sum((nf for ni,nf in emb_szs))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;n_in = n_emb + n_cont<br>\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for i in layers:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Linear(n_in,i)) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.ReLU(inplace=True))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.BatchNorm1d(i))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Dropout(p))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_in = i<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;layerlist.append(nn.Linear(layers[-1],out_sz))<br>\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;self.layers = nn.Sequential(*layerlist)</font></tt><br><br>\n",
    "6. Define the forward method. Preprocess the embeddings and normalize the continuous variables before passing them through the layers.<br>Use <a href='https://pytorch.org/docs/stable/torch.html#torch.cat'><tt><strong>torch.cat()</strong></tt></a> to combine multiple tensors into one.<br>\n",
    "<tt><font color=black>&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x_cat, x_cont):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;embeddings = []<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for i,e in enumerate(self.embeds):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings.append(e(x_cat[:,i]))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x = torch.cat(embeddings, 1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x = self.emb_drop(x)<br>\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_cont = self.bn_cont(x_cont)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x = torch.cat([x, x_cont], 1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x = self.layers(x)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return x</font></tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we code the TabularModel, here is the picture again\n",
    "\n",
    "![title](images/category-embedding.png)\n",
    "Image Source: https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        ## emb_szs: list of tuples: each categorical variable size is paired with an embedding size\n",
    "        ## n_cont: int: number of continuous variables\n",
    "        ## out_sz: int: output size\n",
    "        ## layers: list of ints: layer sizes\n",
    "        ## p: float: dropout probability for each layer (for simplicity we'll use the same value throughout)\n",
    "                                              \n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        \n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularModel(emb_szs, conts.shape[1], 1, [1000,500], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(24, 12)\n",
       "    (1): Embedding(2, 1)\n",
       "    (2): Embedding(7, 4)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=23, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function & optimizer\n",
    "PyTorch does not offer a built-in <a href='https://en.wikipedia.org/wiki/Root-mean-square_deviation'>RMSE Loss</a> function, and it would be nice to see this in place of MSE.<br>\n",
    "For this reason, we'll simply apply the <tt>torch.sqrt()</tt> function to the output of MSELoss during training.\n",
    "\n",
    "Here are the various Loss functions:\n",
    "\n",
    "Regression Loss Functions\n",
    "* Mean Squared Error Loss\n",
    "* Mean Squared Logarithmic Error Loss\n",
    "* Mean Absolute Error Loss\n",
    "\n",
    "Binary Classification Loss Functions\n",
    "* Binary Cross-Entropy\n",
    "* Hinge Loss\n",
    "* Squared Hinge Loss\n",
    "\n",
    "Multi-Class Classification Loss Functions\n",
    "* Multi-Class Cross-Entropy Loss\n",
    "* Sparse Multiclass Cross-Entropy Loss\n",
    "* Kullback Leibler Divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # we'll convert this to RMSE later\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "test_size = int(batch_size * .2)\n",
    "\n",
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "con_train = conts[:batch_size-test_size]\n",
    "con_test = conts[batch_size-test_size:batch_size]\n",
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Expect this to take 30 minutes or more! We've added code to tell us the duration at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1  loss: 12.41653824\n",
      "epoch:  26  loss: 10.21514034\n",
      "epoch:  51  loss: 9.69560623\n",
      "epoch:  76  loss: 8.39893055\n",
      "epoch: 101  loss: 5.76374674\n",
      "epoch: 126  loss: 3.72212243\n",
      "epoch: 151  loss: 3.72262764\n",
      "epoch: 176  loss: 3.53131747\n",
      "epoch: 201  loss: 3.44463754\n",
      "epoch: 226  loss: 3.42164469\n",
      "epoch: 251  loss: 3.29469633\n",
      "epoch: 276  loss: 3.27677751\n",
      "epoch: 301  loss: 3.23937273\n",
      "epoch: 326  loss: 3.18705082\n",
      "epoch: 351  loss: 3.15754247\n",
      "epoch: 376  loss: 3.13695574\n",
      "epoch: 400  loss: 3.08489323\n",
      "\n",
      "Duration: 459 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 400\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model(cat_train, con_train)\n",
    "    loss = torch.sqrt(criterion(y_pred, y_train)) # RMSE\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # a neat trick to save screen space:\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model\n",
    "Here we want to run the entire test set through the model, and compare it to the known labels.<br>\n",
    "For this step we don't want to update weights and biases, so we set <tt>torch.no_grad()</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.52004981\n"
     ]
    }
   ],
   "source": [
    "# TO EVALUATE THE ENTIRE TEST SET\n",
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, con_test)\n",
    "    loss = torch.sqrt(criterion(y_val, y_test))\n",
    "print(f'RMSE: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that on average, predicted values are within &plusmn;$3.31 of the actual value.\n",
    "\n",
    "Now let's look at the first 50 predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTED   ACTUAL     DIFF\n",
      " 1.   6.2244   4.5000   1.7244\n",
      " 2.   7.5593   7.3000   0.2593\n",
      " 3.   6.7583   6.1000   0.6583\n",
      " 4.   7.6676   8.5000   0.8324\n",
      " 5.   4.2858   3.7000   0.5858\n",
      " 6.   9.1208  11.3000   2.1792\n",
      " 7.   8.4581   6.9000   1.5581\n",
      " 8.  11.0709  11.7000   0.6291\n",
      " 9.   7.1542   6.5000   0.6542\n",
      "10.   8.0238   7.3000   0.7238\n",
      "11.   9.4850  10.9000   1.4150\n",
      "12.   4.3692   6.5000   2.1308\n",
      "13.   9.9410  10.5000   0.5590\n",
      "14.   8.5321  12.1000   3.5679\n",
      "15.   8.3157   8.1000   0.2157\n",
      "16.   5.4487   4.9000   0.5487\n",
      "17.  19.6535  16.9000   2.7535\n",
      "18.   8.0682  13.3000   5.2318\n",
      "19.  18.7062  27.6000   8.8938\n",
      "20.   7.8935   5.7000   2.1935\n",
      "21.  11.4199  15.7000   4.2801\n",
      "22.   5.2447   4.5000   0.7447\n",
      "23.   4.7439   5.3000   0.5561\n",
      "24.   5.6191   5.7000   0.0809\n",
      "25.   4.6106   4.9000   0.2894\n",
      "26.  14.7308  12.9000   1.8308\n",
      "27.   6.2036  16.9000  10.6964\n",
      "28.   6.9158   6.5000   0.4158\n",
      "29.   5.2465   5.3000   0.0535\n",
      "30.   9.9661   8.1000   1.8661\n",
      "31.   6.8984   7.3000   0.4016\n",
      "32.   5.1420  16.9000  11.7580\n",
      "33.   4.8756   5.3000   0.4244\n",
      "34.   7.1827   8.5000   1.3173\n",
      "35.   8.4212   6.5000   1.9212\n",
      "36.   6.1631   4.1000   2.0631\n",
      "37.   4.8587   3.7000   1.1587\n",
      "38.  14.9073  15.7000   0.7927\n",
      "39.   6.1434   4.1000   2.0434\n",
      "40.  11.4864   8.5000   2.9864\n",
      "41.   5.5238   7.7000   2.1762\n",
      "42.   5.9089   7.7000   1.7911\n",
      "43.   6.4684   6.5000   0.0316\n",
      "44.   5.2926   3.3000   1.9926\n",
      "45.   5.4989   4.9000   0.5989\n",
      "46.   9.6076  10.1000   0.4924\n",
      "47.  13.3342  12.5000   0.8342\n",
      "48.   4.2082   4.5000   0.2918\n",
      "49.   6.6100  10.5000   3.8900\n",
      "50.   4.4623   5.3000   0.8377\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(50):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while many predictions were off by a few cents, some were off by \\\\$11.00. Feel free to change the batch size, test size, and number of epochs to obtain a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "We can save a trained model to a file in case we want to come back later and feed new data through it. The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwriting a previously saved model with an untrained one.<br>For more information visit <a href='https://pytorch.org/tutorials/beginner/saving_loading_models.html'>https://pytorch.org/tutorials/beginner/saving_loading_models.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to save the model only after the training has happened!\n",
    "if len(losses) == epochs:\n",
    "    torch.save(model.state_dict(), 'TaxiFareRegrModel.pt')\n",
    "else:\n",
    "    print('Model has not been trained. Consider loading a trained model instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a saved model (starting from scratch)\n",
    "We can load the trained weights and biases from a saved model. If we've just opened the notebook, we'll have to run standard imports and function definitions. To demonstrate, restart the kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def haversine_distance(df, lat_1, long_1, lat_2, long_2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    earth_r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi_1 = np.radians(df[lat_1])\n",
    "    phi_2 = np.radians(df[lat_2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat_2]-df[lat_1])\n",
    "    delta_lambda = np.radians(df[long_2]-df[long_1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    k_distance = (earth_r * c) # in kilometers\n",
    "    m_distance = k_distance * 0.621371 # in miles\n",
    "\n",
    "    return m_distance\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model. Before we can load the saved settings, we need to instantiate our TabularModel with the parameters we used before (embedding sizes, number of continuous columns, output size, layer sizes, and dropout layer p-value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(24, 12), (2, 1), (7, 4)]\n",
    "model2 = TabularModel(emb_szs, 6, 1, [1000,500], p=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is set up, loading the saved settings is a snap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(24, 12)\n",
       "    (1): Embedding(2, 1)\n",
       "    (2): Embedding(7, 4)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=23, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('TaxiFareRegrModel.pt'));\n",
    "model2.eval() # be sure to run this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a function that takes in new parameters from the user, performs all of the preprocessing steps above, and passes the new data through our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(mdl): # pass in the name of the new model\n",
    "    # INPUT NEW DATA\n",
    "    plat = float(input('What is the pickup latitude?  '))\n",
    "    plong = float(input('What is the pickup longitude? '))\n",
    "    dlat = float(input('What is the dropoff latitude?  '))\n",
    "    dlong = float(input('What is the dropoff longitude? '))\n",
    "    psngr = int(input('How many passengers? '))\n",
    "    dt = input('What is the pickup date and time?\\nFormat as YYYY-MM-DD HH:MM:SS     ')\n",
    "    \n",
    "    # PREPROCESS THE DATA\n",
    "    dfx_dict = {'pickup_latitude':plat,'pickup_longitude':plong,'dropoff_latitude':dlat,\n",
    "         'dropoff_longitude':dlong,'passenger_count':psngr,'EDTdate':dt}\n",
    "    dfx = pd.DataFrame(dfx_dict, index=[0])\n",
    "    dfx['dist_km'] = haversine_distance(dfx,'pickup_latitude', 'pickup_longitude',\n",
    "                                        'dropoff_latitude', 'dropoff_longitude')\n",
    "    dfx['EDTdate'] = pd.to_datetime(dfx['EDTdate'])\n",
    "    \n",
    "    # We can skip the .astype(category) step since our fields are small,\n",
    "    # and encode them right away\n",
    "    dfx['Hour'] = dfx['EDTdate'].dt.hour\n",
    "    dfx['AMorPM'] = np.where(dfx['Hour']<12,0,1) \n",
    "    dfx['Weekday'] = dfx['EDTdate'].dt.strftime(\"%a\")\n",
    "    dfx['Weekday'] = dfx['Weekday'].replace(['Fri','Mon','Sat','Sun','Thu','Tue','Wed'],\n",
    "                                            [0,1,2,3,4,5,6]).astype('int64')\n",
    "    # CREATE CAT AND CONT TENSORS\n",
    "    cat_cols = ['Hour', 'AMorPM', 'Weekday']\n",
    "    cont_cols = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "                 'dropoff_longitude', 'passenger_count', 'dist_km']\n",
    "    xcats = np.stack([dfx[col].values for col in cat_cols], 1)\n",
    "    xcats = torch.tensor(xcats, dtype=torch.int64)\n",
    "    xconts = np.stack([dfx[col].values for col in cont_cols], 1)\n",
    "    xconts = torch.tensor(xconts, dtype=torch.float)\n",
    "    \n",
    "    # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP\n",
    "    with torch.no_grad():\n",
    "        z = mdl(xcats, xconts)\n",
    "    print(f'\\nThe predicted fare amount is ${z.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed new data through the trained model\n",
    "For convenience, here are the max and min values for each of the variables:\n",
    "<table style=\"display: inline-block\">\n",
    "<tr><th>Column</th><th>Minimum</th><th>Maximum</th></tr>\n",
    "<tr><td>pickup_latitude</td><td>40</td><td>41</td></tr>\n",
    "<tr><td>pickup_longitude</td><td>-74.5</td><td>-73.3</td></tr>\n",
    "<tr><td>dropoff_latitude</td><td>40</td><td>41</td></tr>\n",
    "<tr><td>dropoff_longitude</td><td>-74.5</td><td>-73.3</td></tr>\n",
    "<tr><td>passenger_count</td><td>1</td><td>5</td></tr>\n",
    "<tr><td>EDTdate</td><td>2010-04-11 00:00:00</td><td>2010-04-24 23:59:42</td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Use caution!</strong> The distance between 1 degree of latitude (from 40 to 41) is 111km (69mi) and between 1 degree of longitude (from -73 to -74) is 85km (53mi). The longest cab ride in the dataset spanned a difference of only 0.243 degrees latitude and 0.284 degrees longitude. The mean difference for both latitude and longitude was about 0.02. To get a fair prediction, use values that fall close to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the pickup latitude?  40.5\n",
      "What is the pickup longitude? -73.9\n",
      "What is the dropoff latitude?  40.52\n",
      "What is the dropoff longitude? -73.92\n",
      "How many passengers? 2\n",
      "What is the pickup date and time?\n",
      "Format as YYYY-MM-DD HH:MM:SS     2010-04-15 16:00:00\n",
      "\n",
      "The predicted fare amount is $10.83\n"
     ]
    }
   ],
   "source": [
    "z = test_data(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
